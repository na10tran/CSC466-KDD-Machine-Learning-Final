{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2\n",
    "\n",
    "## Bayesian Classifier and Feature Importance\n",
    "\n",
    "**Note:** Exercises can be autograded and count towards your lab and assignment score. Problems are graded for participation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "home = str(Path.home()) # all other paths are relative to this path. change to something else if this is not the case on your system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NO NEED TO EDIT ####\n",
    "from pathlib import Path\n",
    "home = str(Path.home()) # all other paths are relative to this path. change to something else if this is not the case on your system\n",
    "REPO = f\"{home}/csc-466-student\"\n",
    "NOTEBOOK = \"Final\"\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from importlib import import_module\n",
    "helper = import_module(f'{NOTEBOOK}_helper')\n",
    "\n",
    "#### NO NEED TO EDIT ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we are going to first implement an empirical naive bayesian classifier, then implement feature importance measures and apply it to a dataset, and finally, we will examine the affect of modifying the priors.\n",
    "\n",
    "For developing this lab, we can use famous Titanic Kaggle dataset. Description of the data is found https://www.kaggle.com/c/titanic/data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_date</th>\n",
       "      <th>team_1</th>\n",
       "      <th>team_2</th>\n",
       "      <th>t1_points</th>\n",
       "      <th>t2_points</th>\n",
       "      <th>t1_world_rank</th>\n",
       "      <th>t2_world_rank</th>\n",
       "      <th>t1_h2h_win_perc</th>\n",
       "      <th>t2_h2h_win_perc</th>\n",
       "      <th>winner</th>\n",
       "      <th>...</th>\n",
       "      <th>t2_player5_dpr</th>\n",
       "      <th>t2_player5_spr</th>\n",
       "      <th>t2_player5_opk_ratio</th>\n",
       "      <th>t2_player5_opk_rating</th>\n",
       "      <th>t2_player5_wins_perc_after_fk</th>\n",
       "      <th>t2_player5_fk_perc_in_wins</th>\n",
       "      <th>t2_player5_multikill_perc</th>\n",
       "      <th>t2_player5_rating_at_least_one_perc</th>\n",
       "      <th>t2_player5_is_sniper</th>\n",
       "      <th>t2_player5_clutch_win_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>CLOUD9</td>\n",
       "      <td>HELLRAISERS</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>t2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.147018</td>\n",
       "      <td>0.528</td>\n",
       "      <td>False</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>IMMORTALS</td>\n",
       "      <td>G2</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>t2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.135810</td>\n",
       "      <td>0.438</td>\n",
       "      <td>True</td>\n",
       "      <td>0.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>MOUSESPORTS</td>\n",
       "      <td>IMMORTALS</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>t1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.129</td>\n",
       "      <td>0.148257</td>\n",
       "      <td>0.526</td>\n",
       "      <td>False</td>\n",
       "      <td>0.733333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>DIGNITAS</td>\n",
       "      <td>G2</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>t1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.739</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.135810</td>\n",
       "      <td>0.438</td>\n",
       "      <td>True</td>\n",
       "      <td>0.406250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-12-18</td>\n",
       "      <td>OPTIC</td>\n",
       "      <td>HELLRAISERS</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>t1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.05</td>\n",
       "      <td>0.92</td>\n",
       "      <td>0.733</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.147018</td>\n",
       "      <td>0.528</td>\n",
       "      <td>False</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 170 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   match_date       team_1       team_2  t1_points  t2_points  t1_world_rank  \\\n",
       "0  2016-12-18       CLOUD9  HELLRAISERS         13         16              9   \n",
       "1  2016-12-18    IMMORTALS           G2         17         19             13   \n",
       "2  2016-12-18  MOUSESPORTS    IMMORTALS         16          3             12   \n",
       "3  2016-12-18     DIGNITAS           G2         16          9              6   \n",
       "4  2016-12-18        OPTIC  HELLRAISERS         16         10              4   \n",
       "\n",
       "   t2_world_rank  t1_h2h_win_perc  t2_h2h_win_perc winner  ...  \\\n",
       "0             20         0.500000         0.500000     t2  ...   \n",
       "1             10         0.500000         0.500000     t2  ...   \n",
       "2             13         0.500000         0.500000     t1  ...   \n",
       "3             10         0.416667         0.583333     t1  ...   \n",
       "4             20         0.500000         0.500000     t1  ...   \n",
       "\n",
       "   t2_player5_dpr  t2_player5_spr  t2_player5_opk_ratio  \\\n",
       "0            0.63            0.10                  1.05   \n",
       "1            0.69            0.09                  0.85   \n",
       "2            0.67            0.07                  0.79   \n",
       "3            0.69            0.09                  0.85   \n",
       "4            0.63            0.10                  1.05   \n",
       "\n",
       "   t2_player5_opk_rating  t2_player5_wins_perc_after_fk  \\\n",
       "0                   0.92                          0.733   \n",
       "1                   0.96                          0.739   \n",
       "2                   0.94                          0.723   \n",
       "3                   0.96                          0.739   \n",
       "4                   0.92                          0.733   \n",
       "\n",
       "   t2_player5_fk_perc_in_wins  t2_player5_multikill_perc  \\\n",
       "0                       0.104                   0.147018   \n",
       "1                       0.147                   0.135810   \n",
       "2                       0.129                   0.148257   \n",
       "3                       0.147                   0.135810   \n",
       "4                       0.104                   0.147018   \n",
       "\n",
       "   t2_player5_rating_at_least_one_perc  t2_player5_is_sniper  \\\n",
       "0                                0.528                 False   \n",
       "1                                0.438                  True   \n",
       "2                                0.526                 False   \n",
       "3                                0.438                  True   \n",
       "4                                0.528                 False   \n",
       "\n",
       "   t2_player5_clutch_win_perc  \n",
       "0                    0.666667  \n",
       "1                    0.406250  \n",
       "2                    0.733333  \n",
       "3                    0.406250  \n",
       "4                    0.666667  \n",
       "\n",
       "[5 rows x 170 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "cs = pd.read_csv(\n",
    "    f\"csgo_games.csv\"\n",
    ")\n",
    "cs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need a few columns, and I will also perform some preprocessing for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique stats available\n",
      "['rating', 'impact', 'kdr', 'dmr', 'kpr', 'apr', 'dpr', 'spr', 'opk_ratio', 'opk_rating', 'wins_perc_after_fk', 'fk_perc_in_wins', 'multikill_perc', 'rating_at_least_one_perc', 'is_sniper', 'clutch_win_perc']\n",
      "A list of all players on both teams\n",
      "['t1_player1', 't1_player2', 't1_player3', 't1_player4', 't1_player5', 't2_player1', 't2_player2', 't2_player3', 't2_player4', 't2_player5']\n",
      "All stats for player 2 of team 1\n",
      "['t1_player2_rating', 't1_player2_impact', 't1_player2_kdr', 't1_player2_dmr', 't1_player2_kpr', 't1_player2_apr', 't1_player2_dpr', 't1_player2_spr', 't1_player2_opk_ratio', 't1_player2_opk_rating', 't1_player2_wins_perc_after_fk', 't1_player2_fk_perc_in_wins', 't1_player2_multikill_perc', 't1_player2_rating_at_least_one_perc', 't1_player2_is_sniper', 't1_player2_clutch_win_perc']\n"
     ]
    }
   ],
   "source": [
    "allStats = ['rating','impact','kdr','dmr','kpr','apr','dpr','spr','opk_ratio','opk_rating','wins_perc_after_fk',\n",
    "            'fk_perc_in_wins','multikill_perc','rating_at_least_one_perc','is_sniper','clutch_win_perc']\n",
    "print('All unique stats available')\n",
    "print(allStats)\n",
    "\n",
    "allPlayers = helper.get_players()\n",
    "print('A list of all players on both teams')\n",
    "print(allPlayers)\n",
    "\n",
    "p2All = helper.get_player_stats(1, 2)\n",
    "print('All stats for player 2 of team 1')\n",
    "print(p2All)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       1.0\n",
       "3       1.0\n",
       "4       1.0\n",
       "       ... \n",
       "3782    0.0\n",
       "3783    0.0\n",
       "3784    0.0\n",
       "3785    0.0\n",
       "3786    1.0\n",
       "Name: winner, Length: 3787, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for player in allPlayers:\n",
    "    cs[player + \"_is_sniper\"] = cs[player + \"_is_sniper\"].astype(int)\n",
    "\n",
    "cs.winner.map(dict(t1=1, t2=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All stats for player 1 on team 1\n",
      "['t1_player1_rating', 't1_player1_impact', 't1_player1_kdr', 't1_player1_dmr', 't1_player1_kpr', 't1_player1_apr', 't1_player1_dpr', 't1_player1_spr', 't1_player1_opk_ratio', 't1_player1_opk_rating', 't1_player1_wins_perc_after_fk', 't1_player1_fk_perc_in_wins', 't1_player1_multikill_perc', 't1_player1_rating_at_least_one_perc', 't1_player1_is_sniper', 't1_player1_clutch_win_perc']\n",
      "Rating for each player on both teams\n",
      "['t1_player1_rating', 't1_player2_rating', 't1_player3_rating', 't1_player4_rating', 't1_player5_rating', 't2_player1_rating', 't2_player2_rating', 't2_player3_rating', 't2_player4_rating', 't2_player5_rating']\n"
     ]
    }
   ],
   "source": [
    "p1Stats = ['t1_player1_' + stat for stat in allStats]\n",
    "print('All stats for player 1 on team 1')\n",
    "print(p1Stats)\n",
    "\n",
    "allRating = []\n",
    "for player in allPlayers:\n",
    "    allRating.append(player + '_rating')\n",
    "print('Rating for each player on both teams')\n",
    "print(allRating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all players' stats organized by each stat\n",
      "{'rating': ['t1_player1_rating', 't1_player2_rating', 't1_player3_rating', 't1_player4_rating', 't1_player5_rating', 't2_player1_rating', 't2_player2_rating', 't2_player3_rating', 't2_player4_rating', 't2_player5_rating'], 'impact': ['t1_player1_impact', 't1_player2_impact', 't1_player3_impact', 't1_player4_impact', 't1_player5_impact', 't2_player1_impact', 't2_player2_impact', 't2_player3_impact', 't2_player4_impact', 't2_player5_impact'], 'kdr': ['t1_player1_kdr', 't1_player2_kdr', 't1_player3_kdr', 't1_player4_kdr', 't1_player5_kdr', 't2_player1_kdr', 't2_player2_kdr', 't2_player3_kdr', 't2_player4_kdr', 't2_player5_kdr'], 'dmr': ['t1_player1_dmr', 't1_player2_dmr', 't1_player3_dmr', 't1_player4_dmr', 't1_player5_dmr', 't2_player1_dmr', 't2_player2_dmr', 't2_player3_dmr', 't2_player4_dmr', 't2_player5_dmr'], 'kpr': ['t1_player1_kpr', 't1_player2_kpr', 't1_player3_kpr', 't1_player4_kpr', 't1_player5_kpr', 't2_player1_kpr', 't2_player2_kpr', 't2_player3_kpr', 't2_player4_kpr', 't2_player5_kpr'], 'apr': ['t1_player1_apr', 't1_player2_apr', 't1_player3_apr', 't1_player4_apr', 't1_player5_apr', 't2_player1_apr', 't2_player2_apr', 't2_player3_apr', 't2_player4_apr', 't2_player5_apr'], 'dpr': ['t1_player1_dpr', 't1_player2_dpr', 't1_player3_dpr', 't1_player4_dpr', 't1_player5_dpr', 't2_player1_dpr', 't2_player2_dpr', 't2_player3_dpr', 't2_player4_dpr', 't2_player5_dpr'], 'spr': ['t1_player1_spr', 't1_player2_spr', 't1_player3_spr', 't1_player4_spr', 't1_player5_spr', 't2_player1_spr', 't2_player2_spr', 't2_player3_spr', 't2_player4_spr', 't2_player5_spr'], 'opk_ratio': ['t1_player1_opk_ratio', 't1_player2_opk_ratio', 't1_player3_opk_ratio', 't1_player4_opk_ratio', 't1_player5_opk_ratio', 't2_player1_opk_ratio', 't2_player2_opk_ratio', 't2_player3_opk_ratio', 't2_player4_opk_ratio', 't2_player5_opk_ratio'], 'opk_rating': ['t1_player1_opk_rating', 't1_player2_opk_rating', 't1_player3_opk_rating', 't1_player4_opk_rating', 't1_player5_opk_rating', 't2_player1_opk_rating', 't2_player2_opk_rating', 't2_player3_opk_rating', 't2_player4_opk_rating', 't2_player5_opk_rating'], 'wins_perc_after_fk': ['t1_player1_wins_perc_after_fk', 't1_player2_wins_perc_after_fk', 't1_player3_wins_perc_after_fk', 't1_player4_wins_perc_after_fk', 't1_player5_wins_perc_after_fk', 't2_player1_wins_perc_after_fk', 't2_player2_wins_perc_after_fk', 't2_player3_wins_perc_after_fk', 't2_player4_wins_perc_after_fk', 't2_player5_wins_perc_after_fk'], 'fk_perc_in_wins': ['t1_player1_fk_perc_in_wins', 't1_player2_fk_perc_in_wins', 't1_player3_fk_perc_in_wins', 't1_player4_fk_perc_in_wins', 't1_player5_fk_perc_in_wins', 't2_player1_fk_perc_in_wins', 't2_player2_fk_perc_in_wins', 't2_player3_fk_perc_in_wins', 't2_player4_fk_perc_in_wins', 't2_player5_fk_perc_in_wins'], 'multikill_perc': ['t1_player1_multikill_perc', 't1_player2_multikill_perc', 't1_player3_multikill_perc', 't1_player4_multikill_perc', 't1_player5_multikill_perc', 't2_player1_multikill_perc', 't2_player2_multikill_perc', 't2_player3_multikill_perc', 't2_player4_multikill_perc', 't2_player5_multikill_perc'], 'rating_at_least_one_perc': ['t1_player1_rating_at_least_one_perc', 't1_player2_rating_at_least_one_perc', 't1_player3_rating_at_least_one_perc', 't1_player4_rating_at_least_one_perc', 't1_player5_rating_at_least_one_perc', 't2_player1_rating_at_least_one_perc', 't2_player2_rating_at_least_one_perc', 't2_player3_rating_at_least_one_perc', 't2_player4_rating_at_least_one_perc', 't2_player5_rating_at_least_one_perc'], 'is_sniper': ['t1_player1_is_sniper', 't1_player2_is_sniper', 't1_player3_is_sniper', 't1_player4_is_sniper', 't1_player5_is_sniper', 't2_player1_is_sniper', 't2_player2_is_sniper', 't2_player3_is_sniper', 't2_player4_is_sniper', 't2_player5_is_sniper'], 'clutch_win_perc': ['t1_player1_clutch_win_perc', 't1_player2_clutch_win_perc', 't1_player3_clutch_win_perc', 't1_player4_clutch_win_perc', 't1_player5_clutch_win_perc', 't2_player1_clutch_win_perc', 't2_player2_clutch_win_perc', 't2_player3_clutch_win_perc', 't2_player4_clutch_win_perc', 't2_player5_clutch_win_perc']}\n"
     ]
    }
   ],
   "source": [
    "allPlayersPerStat = helper.get_players_per_stat()\n",
    "print('List of all players\\' stats organized by each stat')\n",
    "print(allPlayersPerStat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p1All' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m p1 \u001b[38;5;241m=\u001b[39m cs\u001b[38;5;241m.\u001b[39mloc[:,\u001b[43mp1All\u001b[49m]\n\u001b[1;32m      2\u001b[0m rating \u001b[38;5;241m=\u001b[39m cs\u001b[38;5;241m.\u001b[39mloc[:,allRating]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBefore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p1All' is not defined"
     ]
    }
   ],
   "source": [
    "p1 = cs.loc[:,p1All]\n",
    "rating = cs.loc[:,allRating]\n",
    "print('Before')\n",
    "display(cs.head())\n",
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['t1_player1', 't1_player2', 't1_player3', 't1_player4', 't1_player5'], ['t2_player1', 't2_player2', 't2_player3', 't2_player4', 't2_player5'])\n"
     ]
    }
   ],
   "source": [
    "allPlayersPerStat = helper.get_players_per_stat()\n",
    "teams = helper.get_teams()\n",
    "print(teams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_norm = cs\n",
    "teamStats = ['points', 'world_rank', 'h2h_win_perc']\n",
    "for stat in teamStats:\n",
    "    cs_norm[stat] = cs_norm['t1_' + stat] - cs_norm['t2_' + stat]\n",
    "    cs_norm = cs_norm.drop('t1_' + stat, axis = 1)\n",
    "    cs_norm = cs_norm.drop('t2_' + stat, axis = 1)\n",
    "\n",
    "for stat in allPlayersPerStat.keys():\n",
    "    t1 = [p + \"_\" + stat for p in teams[0]]\n",
    "    t2 = [p + \"_\" + stat for p in teams[1]]\n",
    "    cs_norm[stat] = (np.sum(cs_norm[t1], axis = 1) - np.sum(cs_norm[t2], axis = 1))\n",
    "    cs_norm = cs_norm.drop(t1, axis = 1)\n",
    "    cs_norm = cs_norm.drop(t2, axis = 1)\n",
    "\n",
    "cs_norm = cs_norm.drop(['match_date', 'team_1', 'team_2'], axis = 1)\n",
    "cs_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1\n",
    "Create a function to determine the prior probability of ALL the classes in ``y``. The result must be in the form of a Python dictionary such as ``priors = {'Survived=0': 0.4, 'Survived=1': 0.6}``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Survived=0': 0.6161616161616161, 'Survived=1': 0.3838383838383838}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "survived_priors = helper.compute_priors(titanic_df['Survived'])\n",
    "survived_priors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Age=0': 0.06958473625140292,\n",
       " 'Age=10': 0.11447811447811448,\n",
       " 'Age=20': 0.44556677890011226,\n",
       " 'Age=30': 0.18742985409652077,\n",
       " 'Age=40': 0.09988776655443322,\n",
       " 'Age=50': 0.05387205387205387,\n",
       " 'Age=60': 0.02132435465768799,\n",
       " 'Age=70': 0.006734006734006734,\n",
       " 'Age=80': 0.001122334455667789}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helper.compute_priors(titanic_df['Age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Age'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_example = titanic_df['Age']\n",
    "y_example.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2\n",
    "Create a function to calculate the specific class conditional probability. Assume x and y are pd.Series objects. Assume xv and yv are specific values. This function should return $\\Pr(x==xv|y==yv)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14754098360655737"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = helper.specific_class_conditional(titanic_df['Sex'],'female',titanic_df['Survived'],0)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.7, pytest-7.1.2, pluggy-1.0.0 -- /opt/tljh/user/bin/python3.9\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/jupyter-waltz@calpoly.edu\n",
      "plugins: clarity-1.0.1, anyio-3.5.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../csc-466-student/tests/test_Lab2.py::test_exercise_2 \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.38s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{NOTEBOOK}.py::test_exercise_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3\n",
    "Now construct a dictionary based data structure that stores all possible class conditional probabilities (e.g., loop through all possible combinations of values). The keys in your dictionary should be of the form \"pclass=1|survived=0\". ``X`` is a ``pd.DataFrame`` object and ``y`` is a ``pd.Series`` object. You can retrieve the name of the series object ``y`` by accessing ``y.name``.\n",
    "\n",
    "Aside: I know it might be a bit annoying to store the key of this dictionary as a string instead of as say a tuple of tuples, but I think the way this prints for folks learning this is reason enough to use strings in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pclass=3|Survived=0': 0.6775956284153005,\n",
       " 'Pclass=3|Survived=1': 0.347953216374269,\n",
       " 'Pclass=1|Survived=0': 0.14571948998178508,\n",
       " 'Pclass=1|Survived=1': 0.39766081871345027,\n",
       " 'Pclass=2|Survived=0': 0.1766848816029144,\n",
       " 'Pclass=2|Survived=1': 0.2543859649122807,\n",
       " 'Sex=male|Survived=0': 0.8524590163934426,\n",
       " 'Sex=male|Survived=1': 0.31871345029239767,\n",
       " 'Sex=female|Survived=0': 0.14754098360655737,\n",
       " 'Sex=female|Survived=1': 0.6812865497076024,\n",
       " 'Age=20|Survived=0': 0.48816029143898,\n",
       " 'Age=20|Survived=1': 0.37719298245614036,\n",
       " 'Age=30|Survived=0': 0.17122040072859745,\n",
       " 'Age=30|Survived=1': 0.2134502923976608,\n",
       " 'Age=50|Survived=0': 0.051001821493624776,\n",
       " 'Age=50|Survived=1': 0.05847953216374269,\n",
       " 'Age=0|Survived=0': 0.04371584699453552,\n",
       " 'Age=0|Survived=1': 0.1111111111111111,\n",
       " 'Age=10|Survived=0': 0.1111111111111111,\n",
       " 'Age=10|Survived=1': 0.11988304093567251,\n",
       " 'Age=40|Survived=0': 0.10018214936247723,\n",
       " 'Age=40|Survived=1': 0.09941520467836257,\n",
       " 'Age=60|Survived=0': 0.023679417122040074,\n",
       " 'Age=60|Survived=1': 0.017543859649122806,\n",
       " 'Age=70|Survived=0': 0.01092896174863388,\n",
       " 'Age=70|Survived=1': 0.0,\n",
       " 'Age=80|Survived=0': 0.0,\n",
       " 'Age=80|Survived=1': 0.0029239766081871343}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = titanic_df.drop(\"Survived\",axis=1)\n",
    "y = titanic_df[\"Survived\"]\n",
    "probs = helper.class_conditional(X,y)\n",
    "probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.7, pytest-7.1.2, pluggy-1.0.0 -- /opt/tljh/user/bin/python3.9\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/jupyter-waltz@calpoly.edu\n",
      "plugins: clarity-1.0.1, anyio-3.5.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../csc-466-student/tests/test_Lab2.py::test_exercise_3 \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.39s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{NOTEBOOK}.py::test_exercise_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4\n",
    "Now you are ready to calculate the posterior probabilities for a given sample. Write and test the following function that returns a dictionary where the keys are of the form \"Survived=0|Pclass=1,Sex=male,Age=60\". Make sure you return 0.5 if the specific combination of values does not exist. ``probs`` and ``priors`` are defined the same as above. ``x`` is a pd.Series object that represents a specific example/sample from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass         3\n",
       "Sex       female\n",
       "Age           20\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = helper.class_conditional(X,y)\n",
    "priors = helper.compute_priors(y)\n",
    "x = titanic_df.drop(\"Survived\",axis=1).loc[2]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Survived=0|Pclass=3,Sex=female,Age=20': 0.46699312907215196,\n",
       " 'Survived=1|Pclass=3,Sex=female,Age=20': 0.533006870927848}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_probs = helper.posteriors(probs,priors,x)\n",
    "post_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is one more test you should check. Your code should return 50/50 if given a value that is not in the empirical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass         3\n",
       "Sex       female\n",
       "Age          200\n",
       "Name: 2, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = titanic_df.drop(\"Survived\",axis=1).loc[2]\n",
    "x = x.copy()\n",
    "x['Age'] = 200\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Survived=0|Pclass=3,Sex=female,Age=200': 0.5,\n",
       " 'Survived=1|Pclass=3,Sex=female,Age=200': 0.5}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_probs = helper.posteriors(probs,priors,x)\n",
    "post_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.7, pytest-7.1.2, pluggy-1.0.0 -- /opt/tljh/user/bin/python3.9\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/jupyter-waltz@calpoly.edu\n",
      "plugins: clarity-1.0.1, anyio-3.5.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../csc-466-student/tests/test_Lab2.py::test_exercise_4 \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.39s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{NOTEBOOK}.py::test_exercise_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5\n",
    "All this is great, but how would you evaluate how we are doing? Create a function called train_test_split that splits our dataframe into a training and testing dataset. To make sure this is done randomly, I have inserted a shuffle into the code for you. The ``test_frac`` is the fraction of the dataset that will be held out for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m Xtrain,ytrain,Xtest,ytest\u001b[38;5;241m=\u001b[39mhelper\u001b[38;5;241m.\u001b[39mtrain_test_split(\u001b[43mX\u001b[49m,y)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m display(Xtrain\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(2)\n",
    "Xtrain,ytrain,Xtest,ytest=helper.train_test_split(X,y)\n",
    "print('Xtrain')\n",
    "display(Xtrain.head())\n",
    "print('ytrain')\n",
    "display(ytrain.head())\n",
    "print('Xtest')\n",
    "display(Xtest.head())\n",
    "print('ytest')\n",
    "display(ytest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.7, pytest-7.1.2, pluggy-1.0.0 -- /opt/tljh/user/bin/python3.9\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/jupyter-waltz@calpoly.edu\n",
      "plugins: clarity-1.0.1, anyio-3.5.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../csc-466-student/tests/test_Lab2.py::test_exercise_5 \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.39s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{NOTEBOOK}.py::test_exercise_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6\n",
    "For this exercise, create a training dataset of size 50% and then using your solutions to previous exercises, find the prediction accuracy for the test dataset. **NOTE:** When/if there is a tie in the posterior probabilities, your code should predict that the passenger passed away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "716    1\n",
       "154    0\n",
       "843    0\n",
       "24     0\n",
       "294    0\n",
       "      ..\n",
       "534    0\n",
       "584    0\n",
       "493    0\n",
       "527    0\n",
       "168    0\n",
       "Name: Survived, Length: 445, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "Xtrain.drop_duplicates().sort_values('Age')\n",
    "ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass       2\n",
       "Sex       male\n",
       "Age         30\n",
       "Name: 213, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.7820224719101123\n",
    "np.random.seed(0)\n",
    "Xtrain,ytrain,Xtest,ytest=helper.train_test_split(X,y)\n",
    "x = Xtest.iloc[0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7820224719101123"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = helper.exercise_6(Xtrain,ytrain,Xtest,ytest)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not bad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.7, pytest-7.1.2, pluggy-1.0.0 -- /opt/tljh/user/bin/python3.9\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/jupyter-waltz@calpoly.edu\n",
      "plugins: clarity-1.0.1, anyio-3.5.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../csc-466-student/tests/test_Lab2.py::test_exercise_6 \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.44s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{NOTEBOOK}.py::test_exercise_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.597752808988764"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = sum(ytest == 0)/ytest.size\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 2:\n",
    "Is that better than guessing all passengers died? What is the accuracy on the test set if you guessed all passengers died?\n",
    "\n",
    "Yes, it is better than predicting that all passengers died. The accuracy of the test set if we guess that all passengers die is (shown above) ~.6 and the accuracy from our prediction model is 0.782\n",
    "\n",
    "**Upload your solution/answer here:** https://canvas.calpoly.edu/courses/89325/assignments/594334"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before proceeding**, make sure you have read the required reading of section 8.5.1 of [this book by Christoph Molnar](https://christophm.github.io/interpretable-ml-book/feature-importance.html). \n",
    "\n",
    "#### Excercise 7. Create a function that returns the test based feature importance for our Bayesian classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pclass': 0.02966292134831472,\n",
       " 'Sex': 0.19550561797752808,\n",
       " 'Age': 0.016853932584269593}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "Xtrain,ytrain,Xtest,ytest=helper.train_test_split(X,y)\n",
    "importances = helper.exercise_7(Xtrain,ytrain,Xtest,ytest)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.7, pytest-7.1.2, pluggy-1.0.0 -- /opt/tljh/user/bin/python3.9\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/jupyter-waltz@calpoly.edu\n",
      "plugins: clarity-1.0.1, anyio-3.5.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../csc-466-student/tests/test_Lab2.py::test_exercise_7 \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 1.93s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{NOTEBOOK}.py::test_exercise_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Excercise 8. Create a function that returns the train based feature importance for our Bayesian classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Pclass': -0.00955056179775271,\n",
       " 'Sex': 0.10471910112359528,\n",
       " 'Age': -0.006516853932584277}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "Xtrain,ytrain,Xtest,ytest=helper.train_test_split(X,y)\n",
    "importances = helper.exercise_8(Xtrain,ytrain,Xtest,ytest)\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.7, pytest-7.1.2, pluggy-1.0.0 -- /opt/tljh/user/bin/python3.9\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /home/jupyter-waltz@calpoly.edu\n",
      "plugins: clarity-1.0.1, anyio-3.5.0\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "../csc-466-student/tests/test_Lab2.py::test_exercise_8 \u001b[32mPASSED\u001b[0m\u001b[32m            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 3.38s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest -vv --diff-symbols {REPO}/tests/test_{NOTEBOOK}.py::test_exercise_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 3. After you implement this, what is the most important feature? \n",
    "\n",
    "The most important feature is Sex. This is demonstrated by sex being the dominant classifier (~10x more important) in both the test based feature importance and the train based feature importance.\n",
    "\n",
    "**Upload your solution/answer here:** https://canvas.calpoly.edu/courses/89325/assignments/594335"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 4: What are the differences between the two sets of importances?\n",
    "\n",
    "The difference between these two sets are where the importance is measured. The test set shows the actual accuracy of our features compared to random values, while the training set primarily shows if any of our data has been overfitted to the training set.\n",
    "\n",
    "**Upload your solution/answer here:** https://canvas.calpoly.edu/courses/89325/assignments/594336"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5: What does a negative value for the importances mean? Consider that it is a very small importance, so what does it say about these features? Consider that question in the context of what you are permuting (training or testing).\n",
    "\n",
    "A negative value of importance means that creating a random sample of values for that feature creates less error than if the original values were to be preserved. The fact that it is near zero means that in the first place, these values are of little importance and might as well be permuted. In the context of permuting both training and testing, it means that the data in the training set is a bad fit for the test set.\n",
    "\n",
    "**Upload your solution/answer here:** https://canvas.calpoly.edu/courses/89325/assignments/594337"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing to add to your brain, are there any correlations between features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.395434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.395434</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pclass       Age\n",
       "Pclass  1.000000 -0.395434\n",
       "Age    -0.395434  1.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.281029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>-0.281029</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Pclass       Age\n",
       "Pclass  1.000000 -0.281029\n",
       "Age    -0.281029  1.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtest.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good job!\n",
    "# Don't forget to push with ./submit.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having trouble with the test cases and the autograder?\n",
    "\n",
    "You can always load up the answers for the autograder. The autograder runs your code and compares your answer to the expected answer. I manually review your code, so there is no need to hide this from you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['exercise_1', 'exercise_2', 'exercise_3', 'exercise_4', 'exercise_5', 'exercise_6', 'exercise_7', 'exercise_8'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "answers = joblib.load(f\"{REPO}/tests/answers_Lab2.joblib\")\n",
    "answers.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7820224719101123"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers['exercise_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
